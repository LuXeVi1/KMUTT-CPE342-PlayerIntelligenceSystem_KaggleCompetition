Mounted at /content/drive
ğŸš€ Torch device: cuda
ğŸ’¾ Available RAM: 8.28 GB / 12.67 GB

ğŸ“Š Class Distribution:
label
0    12429
3     5926
2     5596
1     4407
4     3188
Name: count, dtype: int64

âš–ï¸ Class Weights: {0: np.float64(0.5076192774961783), 1: np.float64(1.0646641916976038), 2: np.float64(1.1274481772694782), 3: np.float64(1.4316314953483096), 4: np.float64(1.9790464240903387)}
/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.
  original_init(self, **validated_kwargs)
/tmp/ipython-input-3886365022.py:77: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise
  A.GaussNoise(var_limit=(10.0, 30.0), p=0.3),
/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.
Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

ğŸ”„ Extracting training features...
  Processed 0/31546 images...
  Processed 160/31546 images...
  Processed 320/31546 images...
  Processed 480/31546 images...
  Processed 640/31546 images...
  Processed 800/31546 images...
  Processed 960/31546 images...
  Processed 1120/31546 images...
  Processed 1280/31546 images...
  Processed 1440/31546 images...
  Processed 1600/31546 images...
  Processed 1760/31546 images...
  Processed 1920/31546 images...
  Processed 2080/31546 images...
  Processed 2240/31546 images...
  Processed 2400/31546 images...
  Processed 2560/31546 images...
  Processed 2720/31546 images...
  Processed 2880/31546 images...
  Processed 3040/31546 images...
  Processed 3200/31546 images...
  Processed 3360/31546 images...
  Processed 3520/31546 images...
  Processed 3680/31546 images...
  Processed 3840/31546 images...
  Processed 4000/31546 images...
  Processed 4160/31546 images...
  Processed 4320/31546 images...
  Processed 4480/31546 images...
  Processed 4640/31546 images...
  Processed 4800/31546 images...
  Processed 4960/31546 images...
  Processed 5120/31546 images...
  Processed 5280/31546 images...
  Processed 5440/31546 images...
  Processed 5600/31546 images...
  Processed 5760/31546 images...
  Processed 5920/31546 images...
  Processed 6080/31546 images...
  Processed 6240/31546 images...
  Processed 6400/31546 images...
  Processed 6560/31546 images...
  Processed 6720/31546 images...
  Processed 6880/31546 images...
  Processed 7040/31546 images...
  Processed 7200/31546 images...
  Processed 7360/31546 images...
  Processed 7520/31546 images...
  Processed 7680/31546 images...
  Processed 7840/31546 images...
  Processed 8000/31546 images...
  Processed 8160/31546 images...
  Processed 8320/31546 images...
  Processed 8480/31546 images...
  Processed 8640/31546 images...
  Processed 8800/31546 images...
  Processed 8960/31546 images...
  Processed 9120/31546 images...
  Processed 9280/31546 images...
  Processed 9440/31546 images...
  Processed 9600/31546 images...
  Processed 9760/31546 images...
  Processed 9920/31546 images...
  Processed 10080/31546 images...
  Processed 10240/31546 images...
  Processed 10400/31546 images...
  Processed 10560/31546 images...
  Processed 10720/31546 images...
  Processed 10880/31546 images...
  Processed 11040/31546 images...
  Processed 11200/31546 images...
  Processed 11360/31546 images...
  Processed 11520/31546 images...
  Processed 11680/31546 images...
  Processed 11840/31546 images...
  Processed 12000/31546 images...
  Processed 12160/31546 images...
  Processed 12320/31546 images...
  Processed 12480/31546 images...
  Processed 12640/31546 images...
  Processed 12800/31546 images...
  Processed 12960/31546 images...
  Processed 13120/31546 images...
  Processed 13280/31546 images...
  Processed 13440/31546 images...
  Processed 13600/31546 images...
  Processed 13760/31546 images...
  Processed 13920/31546 images...
  Processed 14080/31546 images...
  Processed 14240/31546 images...
  Processed 14400/31546 images...
  Processed 14560/31546 images...
  Processed 14720/31546 images...
  Processed 14880/31546 images...
  Processed 15040/31546 images...
  Processed 15200/31546 images...
  Processed 15360/31546 images...
  Processed 15520/31546 images...
  Processed 15680/31546 images...
  Processed 15840/31546 images...
  Processed 16000/31546 images...
  Processed 16160/31546 images...
  Processed 16320/31546 images...
  Processed 16480/31546 images...
  Processed 16640/31546 images...
  Processed 16800/31546 images...
  Processed 16960/31546 images...
  Processed 17120/31546 images...
  Processed 17280/31546 images...
  Processed 17440/31546 images...
  Processed 17600/31546 images...
  Processed 17760/31546 images...
  Processed 17920/31546 images...
  Processed 18080/31546 images...
  Processed 18240/31546 images...
  Processed 18400/31546 images...
  Processed 18560/31546 images...
  Processed 18720/31546 images...
  Processed 18880/31546 images...
  Processed 19040/31546 images...
  Processed 19200/31546 images...
  Processed 19360/31546 images...
  Processed 19520/31546 images...
  Processed 19680/31546 images...
  Processed 19840/31546 images...
  Processed 20000/31546 images...
  Processed 20160/31546 images...
  Processed 20320/31546 images...
  Processed 20480/31546 images...
  Processed 20640/31546 images...
  Processed 20800/31546 images...
  Processed 20960/31546 images...
  Processed 21120/31546 images...
  Processed 21280/31546 images...
  Processed 21440/31546 images...
  Processed 21600/31546 images...
  Processed 21760/31546 images...
  Processed 21920/31546 images...
  Processed 22080/31546 images...
  Processed 22240/31546 images...
  Processed 22400/31546 images...
  Processed 22560/31546 images...
  Processed 22720/31546 images...
  Processed 22880/31546 images...
  Processed 23040/31546 images...
  Processed 23200/31546 images...
  Processed 23360/31546 images...
  Processed 23520/31546 images...
  Processed 23680/31546 images...
  Processed 23840/31546 images...
  Processed 24000/31546 images...
  Processed 24160/31546 images...
  Processed 24320/31546 images...
  Processed 24480/31546 images...
  Processed 24640/31546 images...
  Processed 24800/31546 images...
  Processed 24960/31546 images...
  Processed 25120/31546 images...
  Processed 25280/31546 images...
  Processed 25440/31546 images...
  Processed 25600/31546 images...
  Processed 25760/31546 images...
  Processed 25920/31546 images...
  Processed 26080/31546 images...
  Processed 26240/31546 images...
  Processed 26400/31546 images...
  Processed 26560/31546 images...
  Processed 26720/31546 images...
  Processed 26880/31546 images...
  Processed 27040/31546 images...
  Processed 27200/31546 images...
  Processed 27360/31546 images...
  Processed 27520/31546 images...
  Processed 27680/31546 images...
  Processed 27840/31546 images...
  Processed 28000/31546 images...
  Processed 28160/31546 images...
  Processed 28320/31546 images...
  Processed 28480/31546 images...
  Processed 28640/31546 images...
  Processed 28800/31546 images...
  Processed 28960/31546 images...
  Processed 29120/31546 images...
  Processed 29280/31546 images...
  Processed 29440/31546 images...
  Processed 29600/31546 images...
  Processed 29760/31546 images...
  Processed 29920/31546 images...
  Processed 30080/31546 images...
  Processed 30240/31546 images...
  Processed 30400/31546 images...
  Processed 30560/31546 images...
  Processed 30720/31546 images...
  Processed 30880/31546 images...
  Processed 31040/31546 images...
  Processed 31200/31546 images...
  Processed 31360/31546 images...
  Processed 31520/31546 images...
âœ… Train features: (31546, 768)

============================================================
ğŸ¯ FOLD 1/5
============================================================
Train: (25236, 768), Val: (6310, 768)
Epoch 1/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 5ms/step - accuracy: 0.4652 - loss: 0.7888 - val_accuracy: 0.7704 - val_loss: 0.5529 - learning_rate: 1.0000e-04
Epoch 2/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.6929 - loss: 0.5583 - val_accuracy: 0.8166 - val_loss: 0.4403 - learning_rate: 1.0000e-04
Epoch 3/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.7654 - loss: 0.4371 - val_accuracy: 0.8396 - val_loss: 0.3443 - learning_rate: 1.0000e-04
Epoch 4/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8050 - loss: 0.3403 - val_accuracy: 0.8566 - val_loss: 0.2654 - learning_rate: 1.0000e-04
Epoch 5/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8269 - loss: 0.2615 - val_accuracy: 0.8631 - val_loss: 0.2062 - learning_rate: 1.0000e-04
Epoch 6/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8446 - loss: 0.2020 - val_accuracy: 0.8716 - val_loss: 0.1616 - learning_rate: 1.0000e-04
Epoch 7/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8554 - loss: 0.1611 - val_accuracy: 0.8769 - val_loss: 0.1303 - learning_rate: 1.0000e-04
Epoch 8/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8631 - loss: 0.1297 - val_accuracy: 0.8808 - val_loss: 0.1093 - learning_rate: 1.0000e-04
Epoch 9/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8643 - loss: 0.1087 - val_accuracy: 0.8807 - val_loss: 0.0948 - learning_rate: 1.0000e-04
Epoch 10/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8724 - loss: 0.0938 - val_accuracy: 0.8803 - val_loss: 0.0846 - learning_rate: 1.0000e-04
Epoch 11/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8792 - loss: 0.0830 - val_accuracy: 0.8851 - val_loss: 0.0766 - learning_rate: 1.0000e-04
Epoch 12/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8856 - loss: 0.0754 - val_accuracy: 0.8823 - val_loss: 0.0726 - learning_rate: 1.0000e-04
Epoch 13/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8806 - loss: 0.0711 - val_accuracy: 0.8886 - val_loss: 0.0680 - learning_rate: 1.0000e-04
Epoch 14/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 3ms/step - accuracy: 0.8836 - loss: 0.0670 - val_accuracy: 0.8895 - val_loss: 0.0648 - learning_rate: 1.0000e-04
Epoch 15/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8917 - loss: 0.0630 - val_accuracy: 0.8886 - val_loss: 0.0635 - learning_rate: 1.0000e-04
Restoring model weights from the end of the best epoch: 15.

âœ… Fold 1 Macro F1: 0.8737

============================================================
ğŸ¯ FOLD 2/5
============================================================
Train: (25237, 768), Val: (6309, 768)
Epoch 1/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 5ms/step - accuracy: 0.4597 - loss: 0.7775 - val_accuracy: 0.7794 - val_loss: 0.5439 - learning_rate: 1.0000e-04
Epoch 2/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.6936 - loss: 0.5486 - val_accuracy: 0.8198 - val_loss: 0.4299 - learning_rate: 1.0000e-04
Epoch 3/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.7672 - loss: 0.4267 - val_accuracy: 0.8455 - val_loss: 0.3317 - learning_rate: 1.0000e-04
Epoch 4/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 3ms/step - accuracy: 0.8012 - loss: 0.3290 - val_accuracy: 0.8597 - val_loss: 0.2530 - learning_rate: 1.0000e-04
Epoch 5/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8239 - loss: 0.2515 - val_accuracy: 0.8692 - val_loss: 0.1945 - learning_rate: 1.0000e-04
Epoch 6/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8378 - loss: 0.1938 - val_accuracy: 0.8754 - val_loss: 0.1534 - learning_rate: 1.0000e-04
Epoch 7/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8535 - loss: 0.1525 - val_accuracy: 0.8808 - val_loss: 0.1244 - learning_rate: 1.0000e-04
Epoch 8/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8607 - loss: 0.1250 - val_accuracy: 0.8786 - val_loss: 0.1048 - learning_rate: 1.0000e-04
Epoch 9/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8663 - loss: 0.1058 - val_accuracy: 0.8827 - val_loss: 0.0910 - learning_rate: 1.0000e-04
Epoch 10/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8745 - loss: 0.0915 - val_accuracy: 0.8856 - val_loss: 0.0811 - learning_rate: 1.0000e-04
Epoch 11/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8760 - loss: 0.0815 - val_accuracy: 0.8845 - val_loss: 0.0754 - learning_rate: 1.0000e-04
Epoch 12/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8837 - loss: 0.0732 - val_accuracy: 0.8889 - val_loss: 0.0698 - learning_rate: 1.0000e-04
Epoch 13/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8847 - loss: 0.0692 - val_accuracy: 0.8924 - val_loss: 0.0667 - learning_rate: 1.0000e-04
Epoch 14/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8812 - loss: 0.0670 - val_accuracy: 0.8889 - val_loss: 0.0642 - learning_rate: 1.0000e-04
Epoch 15/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8858 - loss: 0.0638 - val_accuracy: 0.8909 - val_loss: 0.0628 - learning_rate: 1.0000e-04
Restoring model weights from the end of the best epoch: 15.

âœ… Fold 2 Macro F1: 0.8754

============================================================
ğŸ¯ FOLD 3/5
============================================================
Train: (25237, 768), Val: (6309, 768)
Epoch 1/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11s 5ms/step - accuracy: 0.4473 - loss: 0.7932 - val_accuracy: 0.7733 - val_loss: 0.5489 - learning_rate: 1.0000e-04
Epoch 2/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.7085 - loss: 0.5513 - val_accuracy: 0.8114 - val_loss: 0.4370 - learning_rate: 1.0000e-04
Epoch 3/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.7648 - loss: 0.4324 - val_accuracy: 0.8432 - val_loss: 0.3378 - learning_rate: 1.0000e-04
Epoch 4/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8037 - loss: 0.3331 - val_accuracy: 0.8594 - val_loss: 0.2580 - learning_rate: 1.0000e-04
Epoch 5/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8258 - loss: 0.2538 - val_accuracy: 0.8694 - val_loss: 0.1990 - learning_rate: 1.0000e-04
Epoch 6/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8432 - loss: 0.1948 - val_accuracy: 0.8732 - val_loss: 0.1558 - learning_rate: 1.0000e-04
Epoch 7/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8549 - loss: 0.1537 - val_accuracy: 0.8780 - val_loss: 0.1256 - learning_rate: 1.0000e-04
Epoch 8/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8663 - loss: 0.1243 - val_accuracy: 0.8800 - val_loss: 0.1056 - learning_rate: 1.0000e-04
Epoch 9/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8683 - loss: 0.1048 - val_accuracy: 0.8800 - val_loss: 0.0928 - learning_rate: 1.0000e-04
Epoch 10/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8723 - loss: 0.0918 - val_accuracy: 0.8814 - val_loss: 0.0816 - learning_rate: 1.0000e-04
Epoch 11/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8801 - loss: 0.0797 - val_accuracy: 0.8868 - val_loss: 0.0755 - learning_rate: 1.0000e-04
Epoch 12/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8810 - loss: 0.0744 - val_accuracy: 0.8848 - val_loss: 0.0708 - learning_rate: 1.0000e-04
Epoch 13/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8828 - loss: 0.0687 - val_accuracy: 0.8917 - val_loss: 0.0663 - learning_rate: 1.0000e-04
Epoch 14/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8876 - loss: 0.0648 - val_accuracy: 0.8897 - val_loss: 0.0643 - learning_rate: 1.0000e-04
Epoch 15/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8896 - loss: 0.0624 - val_accuracy: 0.8879 - val_loss: 0.0632 - learning_rate: 1.0000e-04
Restoring model weights from the end of the best epoch: 15.

âœ… Fold 3 Macro F1: 0.8730

============================================================
ğŸ¯ FOLD 4/5
============================================================
Train: (25237, 768), Val: (6309, 768)
Epoch 1/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11s 5ms/step - accuracy: 0.4460 - loss: 0.7799 - val_accuracy: 0.7721 - val_loss: 0.5450 - learning_rate: 1.0000e-04
Epoch 2/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.6982 - loss: 0.5501 - val_accuracy: 0.8215 - val_loss: 0.4303 - learning_rate: 1.0000e-04
Epoch 3/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.7721 - loss: 0.4271 - val_accuracy: 0.8477 - val_loss: 0.3325 - learning_rate: 1.0000e-04
Epoch 4/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8048 - loss: 0.3298 - val_accuracy: 0.8621 - val_loss: 0.2540 - learning_rate: 1.0000e-04
Epoch 5/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8260 - loss: 0.2534 - val_accuracy: 0.8705 - val_loss: 0.1964 - learning_rate: 1.0000e-04
Epoch 6/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8422 - loss: 0.1960 - val_accuracy: 0.8730 - val_loss: 0.1553 - learning_rate: 1.0000e-04
Epoch 7/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8516 - loss: 0.1548 - val_accuracy: 0.8794 - val_loss: 0.1248 - learning_rate: 1.0000e-04
Epoch 8/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8627 - loss: 0.1250 - val_accuracy: 0.8859 - val_loss: 0.1045 - learning_rate: 1.0000e-04
Epoch 9/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8728 - loss: 0.1047 - val_accuracy: 0.8838 - val_loss: 0.0905 - learning_rate: 1.0000e-04
Epoch 10/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8705 - loss: 0.0931 - val_accuracy: 0.8848 - val_loss: 0.0830 - learning_rate: 1.0000e-04
Epoch 11/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8744 - loss: 0.0829 - val_accuracy: 0.8898 - val_loss: 0.0743 - learning_rate: 1.0000e-04
Epoch 12/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8827 - loss: 0.0748 - val_accuracy: 0.8895 - val_loss: 0.0702 - learning_rate: 1.0000e-04
Epoch 13/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8813 - loss: 0.0712 - val_accuracy: 0.8873 - val_loss: 0.0674 - learning_rate: 1.0000e-04
Epoch 14/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8852 - loss: 0.0670 - val_accuracy: 0.8908 - val_loss: 0.0644 - learning_rate: 1.0000e-04
Epoch 15/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8904 - loss: 0.0641 - val_accuracy: 0.8892 - val_loss: 0.0630 - learning_rate: 1.0000e-04
Restoring model weights from the end of the best epoch: 15.

âœ… Fold 4 Macro F1: 0.8774

============================================================
ğŸ¯ FOLD 5/5
============================================================
Train: (25237, 768), Val: (6309, 768)
Epoch 1/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 5ms/step - accuracy: 0.4641 - loss: 0.7782 - val_accuracy: 0.7783 - val_loss: 0.5490 - learning_rate: 1.0000e-04
Epoch 2/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.6909 - loss: 0.5579 - val_accuracy: 0.8331 - val_loss: 0.4369 - learning_rate: 1.0000e-04
Epoch 3/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.7563 - loss: 0.4396 - val_accuracy: 0.8548 - val_loss: 0.3408 - learning_rate: 1.0000e-04
Epoch 4/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8023 - loss: 0.3376 - val_accuracy: 0.8640 - val_loss: 0.2618 - learning_rate: 1.0000e-04
Epoch 5/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8253 - loss: 0.2589 - val_accuracy: 0.8719 - val_loss: 0.2026 - learning_rate: 1.0000e-04
Epoch 6/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8348 - loss: 0.2028 - val_accuracy: 0.8816 - val_loss: 0.1589 - learning_rate: 1.0000e-04
Epoch 7/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8559 - loss: 0.1590 - val_accuracy: 0.8862 - val_loss: 0.1278 - learning_rate: 1.0000e-04
Epoch 8/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8535 - loss: 0.1317 - val_accuracy: 0.8825 - val_loss: 0.1081 - learning_rate: 1.0000e-04
Epoch 9/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8725 - loss: 0.1074 - val_accuracy: 0.8903 - val_loss: 0.0921 - learning_rate: 1.0000e-04
Epoch 10/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8734 - loss: 0.0946 - val_accuracy: 0.8886 - val_loss: 0.0823 - learning_rate: 1.0000e-04
Epoch 11/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8768 - loss: 0.0838 - val_accuracy: 0.8919 - val_loss: 0.0753 - learning_rate: 1.0000e-04
Epoch 12/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8825 - loss: 0.0761 - val_accuracy: 0.8903 - val_loss: 0.0696 - learning_rate: 1.0000e-04
Epoch 13/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8837 - loss: 0.0720 - val_accuracy: 0.8949 - val_loss: 0.0660 - learning_rate: 1.0000e-04
Epoch 14/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 4ms/step - accuracy: 0.8869 - loss: 0.0664 - val_accuracy: 0.8982 - val_loss: 0.0642 - learning_rate: 1.0000e-04
Epoch 15/15
1578/1578 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 3ms/step - accuracy: 0.8904 - loss: 0.0639 - val_accuracy: 0.8963 - val_loss: 0.0614 - learning_rate: 1.0000e-04
Restoring model weights from the end of the best epoch: 15.

âœ… Fold 5 Macro F1: 0.8822

============================================================
ğŸ“Š CROSS-VALIDATION RESULTS
============================================================
Overall Macro F1: 0.8763

Per-fold results:
   fold  f1_macro  best_epoch
0     1  0.873719          15
1     2  0.875416          15
2     3  0.873015          15
3     4  0.877393          15
4     5  0.882187          15

              precision    recall  f1-score   support

           0       0.93      0.95      0.94     12429
           1       0.89      0.87      0.88      4407
           2       0.87      0.83      0.85      5596
           3       0.84      0.84      0.84      5926
           4       0.88      0.86      0.87      3188

    accuracy                           0.89     31546
   macro avg       0.88      0.87      0.88     31546
weighted avg       0.89      0.89      0.89     31546


ğŸš€ Training XGBoost on original features...
âœ… Skipping SMOTE (sufficient samples)
âœ… Saved XGBoost model

ğŸ”® Test Inference...
  Processed 0/25889 test images...
  Processed 160/25889 test images...
  Processed 320/25889 test images...
  Processed 480/25889 test images...
  Processed 640/25889 test images...
  Processed 800/25889 test images...
  Processed 960/25889 test images...
  Processed 1120/25889 test images...
  Processed 1280/25889 test images...
  Processed 1440/25889 test images...
  Processed 1600/25889 test images...
  Processed 1760/25889 test images...
  Processed 1920/25889 test images...
  Processed 2080/25889 test images...
  Processed 2240/25889 test images...
  Processed 2400/25889 test images...
  Processed 2560/25889 test images...
  Processed 2720/25889 test images...
  Processed 2880/25889 test images...
  Processed 3040/25889 test images...
  Processed 3200/25889 test images...
  Processed 3360/25889 test images...
  Processed 3520/25889 test images...
  Processed 3680/25889 test images...
  Processed 3840/25889 test images...
  Processed 4000/25889 test images...
  Processed 4160/25889 test images...
  Processed 4320/25889 test images...
  Processed 4480/25889 test images...
  Processed 4640/25889 test images...
  Processed 4800/25889 test images...
  Processed 4960/25889 test images...
  Processed 5120/25889 test images...
  Processed 5280/25889 test images...
  Processed 5440/25889 test images...
  Processed 5600/25889 test images...
  Processed 5760/25889 test images...
  Processed 5920/25889 test images...
  Processed 6080/25889 test images...
  Processed 6240/25889 test images...
  Processed 6400/25889 test images...
  Processed 6560/25889 test images...
  Processed 6720/25889 test images...
  Processed 6880/25889 test images...
  Processed 7040/25889 test images...
  Processed 7200/25889 test images...
  Processed 7360/25889 test images...
  Processed 7520/25889 test images...
  Processed 7680/25889 test images...
  Processed 7840/25889 test images...
  Processed 8000/25889 test images...
  Processed 8160/25889 test images...
  Processed 8320/25889 test images...
  Processed 8480/25889 test images...
  Processed 8640/25889 test images...
  Processed 8800/25889 test images...
  Processed 8960/25889 test images...
  Processed 9120/25889 test images...
  Processed 9280/25889 test images...
  Processed 9440/25889 test images...
  Processed 9600/25889 test images...
  Processed 9760/25889 test images...
  Processed 9920/25889 test images...
  Processed 10080/25889 test images...
  Processed 10240/25889 test images...
  Processed 10400/25889 test images...
  Processed 10560/25889 test images...
  Processed 10720/25889 test images...
  Processed 10880/25889 test images...
  Processed 11040/25889 test images...
  Processed 11200/25889 test images...
  Processed 11360/25889 test images...
  Processed 11520/25889 test images...
  Processed 11680/25889 test images...
  Processed 11840/25889 test images...
  Processed 12000/25889 test images...
  Processed 12160/25889 test images...
  Processed 12320/25889 test images...
  Processed 12480/25889 test images...
  Processed 12640/25889 test images...
  Processed 12800/25889 test images...
  Processed 12960/25889 test images...
  Processed 13120/25889 test images...
  Processed 13280/25889 test images...
  Processed 13440/25889 test images...
  Processed 13600/25889 test images...
  Processed 13760/25889 test images...
  Processed 13920/25889 test images...
  Processed 14080/25889 test images...
  Processed 14240/25889 test images...
  Processed 14400/25889 test images...
  Processed 14560/25889 test images...
  Processed 14720/25889 test images...
  Processed 14880/25889 test images...
  Processed 15040/25889 test images...
  Processed 15200/25889 test images...
  Processed 15360/25889 test images...
  Processed 15520/25889 test images...
  Processed 15680/25889 test images...
  Processed 15840/25889 test images...
  Processed 16000/25889 test images...
  Processed 16160/25889 test images...
  Processed 16320/25889 test images...
  Processed 16480/25889 test images...
  Processed 16640/25889 test images...
  Processed 16800/25889 test images...
  Processed 16960/25889 test images...
  Processed 17120/25889 test images...
  Processed 17280/25889 test images...
  Processed 17440/25889 test images...
  Processed 17600/25889 test images...
  Processed 17760/25889 test images...
  Processed 17920/25889 test images...
  Processed 18080/25889 test images...
  Processed 18240/25889 test images...
  Processed 18400/25889 test images...
  Processed 18560/25889 test images...
  Processed 18720/25889 test images...
  Processed 18880/25889 test images...
  Processed 19040/25889 test images...
  Processed 19200/25889 test images...
  Processed 19360/25889 test images...
  Processed 19520/25889 test images...
  Processed 19680/25889 test images...
  Processed 19840/25889 test images...
  Processed 20000/25889 test images...
  Processed 20160/25889 test images...
  Processed 20320/25889 test images...
  Processed 20480/25889 test images...
  Processed 20640/25889 test images...
  Processed 20800/25889 test images...
  Processed 20960/25889 test images...
  Processed 21120/25889 test images...
  Processed 21280/25889 test images...
  Processed 21440/25889 test images...
  Processed 21600/25889 test images...
  Processed 21760/25889 test images...
  Processed 21920/25889 test images...
  Processed 22080/25889 test images...
  Processed 22240/25889 test images...
  Processed 22400/25889 test images...
  Processed 22560/25889 test images...
  Processed 22720/25889 test images...
  Processed 22880/25889 test images...
  Processed 23040/25889 test images...
  Processed 23200/25889 test images...
  Processed 23360/25889 test images...
  Processed 23520/25889 test images...
  Processed 23680/25889 test images...
  Processed 23840/25889 test images...
  Processed 24000/25889 test images...
  Processed 24160/25889 test images...
  Processed 24320/25889 test images...
  Processed 24480/25889 test images...
  Processed 24640/25889 test images...
  Processed 24800/25889 test images...
  Processed 24960/25889 test images...
  Processed 25120/25889 test images...
  Processed 25280/25889 test images...
  Processed 25440/25889 test images...
  Processed 25600/25889 test images...
  Processed 25760/25889 test images...
âœ… Test features: (25889, 768)
  Processing TTA (horizontal flip)...
  Averaging fold predictions...
  Getting XGBoost predictions...

============================================================
âœ… PIPELINE COMPLETED!
============================================================
ğŸ“ Saved to: /content/drive/MyDrive/Task4_v4_improved
   Models: 5 fold models + XGBoost
   CV F1:  0.8763
   Submission: /content/drive/MyDrive/Task4_v4_improved/logs/submission_task4_v4.csv

ğŸ® Prediction distribution:
task4
0    6397
1    4983
2    3643
3    7433
4    3433
Name: count, dtype: int64